## üöÄ PROYECTO DATA SCIENTIST/AI DEVELOPER: Aprendizaje Supervisado

Proyecto educativo hecho por:

- [Omar Lengua](https://www.linkedin.com/in/omarlengua/)
- [Max Beltran](https://www.linkedin.com/in/max-beltran/)
- [Cesar Mercado](https://www.linkedin.com/in/cesarmercadohernandez/)
- [Alla Haruty](https://www.linkedin.com/in/allaharuty/)

Este proyecto implementa un modelo de Machine Learning supervisado de clasificaci√≥n que permite predecir la probabilidad de un accidente cerebrovascular (stroke) a partir de un formulario con datos personales o a partir de una im√°genes de resonancia magn√©tica del paciente. 

### ¬øQu√© dataset utilizamos?

- [Dataset inicial](https://www.kaggle.com/datasets/jillanisofttech/brain-stroke-dataset/data)
- [Dataset secundario para implementar data augmentation y tratamiento de datos desbalanceados](https://data.mendeley.com/datasets/x8ygrw87jw/1)
- [Dataset de im√°genes](https://www.kaggle.com/datasets/afridirahman/brain-stroke-ct-image-dataset/data)

Al analizar el dataset inicial notamos un problema importante: los datos estaban fuertemente desbalanceados. Espec√≠ficamente, hab√≠a 4.733 casos sin ictus frente a solo 248 con ictus, lo que representa un serio obst√°culo para entrenar un modelo fiable.

### ¬øPor qu√© decidimos combinar dos datasets?

Debido al desbalanceo del dataset original, sab√≠amos que el modelo podr√≠a tener dificultades para identificar correctamente los casos positivos (ictus), lo que en un contexto de salud podr√≠a tener consecuencias graves. En situaciones como esta, es preferible que el modelo d√© falsos positivos antes que falle al detectar un caso real de ictus.

Por eso, decidimos incorporar datos adicionales de un [segundo dataset](https://data.mendeley.com/datasets/x8ygrw87jw/1), asegur√°ndonos de que tuviera al menos las mismas columnas para facilitar la integraci√≥n. Sin embargo, al combinar ambos conjuntos, el desbalanceo segu√≠a siendo un problema. Para solucionarlo, optamos por a√±adir √∫nicamente las instancias donde stroke = 1 del segundo dataset, incrementando as√≠ el n√∫mero total de casos positivos.

### Resultado de la combinaci√≥n

Con esta estrategia pasamos de tener 248 casos positivos a 783 casos de ictus, mejorando significativamente el equilibrio entre clases. Esto nos proporciona una base m√°s s√≥lida para entrenar un modelo con mejor capacidad predictiva, especialmente en la detecci√≥n de casos positivos.

Para ello, hicimos un an√°lisis exploratorio de datos completo que se puede ver paso a paso en la rama [feature/EDA](https://github.com/alharuty/Proyecto-IX-DS2/tree/feature/EDA) donde cada integrante del equipo estudi√≥ y analiz√≥ el dataset, y finalmente pudimos verificar y considerar un an√°lisis de datos final que lo llamamos EDA.ipyb .

Para el encontrar el mejor modelo y las mejores m√©tricas, seguimos el mismo paso en la rama [feature/model](https://github.com/alharuty/Proyecto-IX-DS2/tree/feature/model) donde cada integrante estudi√≥ y propuso el mejor modelo encontrado. Finalmente elejimos el **modelo RandomForest con optimizaci√≥n de hyperpar√°metros, SMOTE y GridSearch** llamado model.pkl.

‚ÄºÔ∏èTODO: A√±adir captura de las m√©tricas

Adem√°s, como un paso extra y de nivel avanzado, Max pudo entrenar un **modelo de red neuronal (CNN) con PyTorch** llamado cnn_pytorch.pth, para realizar las predicciones mediante las im√°genes.

‚ÄºÔ∏èTODO: A√±adir diagrama de arquitectura

‚ÄºÔ∏èTODO: A√±adir demo

‚ÄºÔ∏èTODO: A√±adir instrucciones para descargar repo

‚ÄºÔ∏èTODO: A√±adir instrucciones para descargar docker

‚ÄºÔ∏èTODO: Link a render si podemos desplegar


Hemos conseguido terminar este proyecto con la metodolog√≠a scrum y un reparto de roles:
- Yo como Scrum Master y desarrolladora Backend con FastApi
- Omar Lengua como desarrollador Frontend con React
- Max como Ingeniero de Datos e Ingeniero de Machine Learning CNN
- Cesar como Ingeniero de Machine Learning RandomForest

Aunque los roles estaban bien definidos, uno de los aspectos m√°s valiosos del proyecto ha sido la colaboraci√≥n transversal. El an√°lisis de datos ha sido un esfuerzo compartido por todo el equipo, permiti√©ndonos tener una comprensi√≥n profunda del conjunto de datos, identificar variables relevantes, realizar limpieza y preprocesamiento, y definir las estrategias de modelado m√°s adecuadas.